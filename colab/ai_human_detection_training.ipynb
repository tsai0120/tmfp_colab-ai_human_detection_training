{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI vs Human æ–‡æœ¬åµæ¸¬æ¨¡å‹è¨“ç·´\n",
        "\n",
        "æœ¬ Notebook ç”¨æ–¼åœ¨ Google Colab ä¸Šè¨“ç·´äº”å€‹æ¨¡å‹ï¼š\n",
        "- TF-IDF + SVM\n",
        "- TF-IDF + Logistic Regression\n",
        "- BERT\n",
        "- RoBERTa + LoRA\n",
        "- Hybrid\n",
        "\n",
        "## ğŸ“‹ ä½¿ç”¨èªªæ˜\n",
        "\n",
        "1. **å•Ÿç”¨ GPU**ï¼šåŸ·è¡Œéšæ®µ â†’ è®Šæ›´åŸ·è¡Œéšæ®µé¡å‹ â†’ é¸æ“‡ GPU\n",
        "2. **ä¸Šå‚³è³‡æ–™**ï¼šåŸ·è¡Œä¸Šå‚³ cellï¼Œé¸æ“‡ `AI_Human.csv`\n",
        "3. **ä¾åºåŸ·è¡Œ**ï¼šæŒ‰ç…§é †åºåŸ·è¡Œå„å€‹ cell\n",
        "4. **ä¸‹è¼‰æ¨¡å‹**ï¼šè¨“ç·´å®Œæˆå¾Œä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç’°å¢ƒè¨­ç½®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£å¿…è¦çš„å¥—ä»¶\n",
        "!pip install -q pandas numpy scikit-learn nltk transformers torch peft tqdm\n",
        "\n",
        "# ä¸‹è¼‰ NLTK è³‡æ–™\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"âœ… ç’°å¢ƒè¨­ç½®å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ä¸Šå‚³è³‡æ–™\n",
        "\n",
        "è«‹ä¸Šå‚³æ‚¨çš„ `AI_Human.csv` æª”æ¡ˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# ä¸Šå‚³æª”æ¡ˆ\n",
        "uploaded = files.upload()\n",
        "\n",
        "# æª¢æŸ¥ä¸Šå‚³çš„æª”æ¡ˆ\n",
        "for filename in uploaded.keys():\n",
        "    print(f'âœ… å·²ä¸Šå‚³: {filename}')\n",
        "    if filename.endswith('.csv'):\n",
        "        # é‡æ–°å‘½åç‚º AI_Human.csvï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
        "        if filename != 'AI_Human.csv':\n",
        "            os.rename(filename, 'AI_Human.csv')\n",
        "            print(f'ğŸ“ å·²é‡æ–°å‘½åç‚º AI_Human.csv')\n",
        "\n",
        "print(\"\\nğŸ“‚ ç•¶å‰ç›®éŒ„æª”æ¡ˆ:\")\n",
        "!ls -lh *.csv 2>/dev/null || echo \"æœªæ‰¾åˆ° CSV æª”æ¡ˆ\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. å»ºç«‹å·¥å…·æ¨¡çµ„\n",
        "\n",
        "### 3.1 é è™•ç†æ¨¡çµ„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile preprocessing.py\n",
        "\"\"\"\n",
        "æ–‡æœ¬é è™•ç†å·¥å…·\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_text(text: str, remove_stopwords: bool = False) -> str:\n",
        "    \"\"\"æ¸…ç†æ–‡æœ¬\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = word_tokenize(text)\n",
        "        text = ' '.join([token for token in tokens if token.lower() not in stop_words])\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_dataframe(df, text_column='text', remove_stopwords=False):\n",
        "    \"\"\"æ‰¹é‡é è™•ç† DataFrame\"\"\"\n",
        "    df = df.copy()\n",
        "    df[text_column] = df[text_column].apply(\n",
        "        lambda x: clean_text(x, remove_stopwords=remove_stopwords)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "print(\"âœ… é è™•ç†æ¨¡çµ„å·²å»ºç«‹\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from preprocessing import preprocess_dataframe\n",
        "\n",
        "print(\"ğŸš€ é–‹å§‹è¨“ç·´ TF-IDF + SVM æ¨¡å‹...\")\n",
        "\n",
        "# è¼‰å…¥è³‡æ–™\n",
        "print(\"ğŸ“‚ è¼‰å…¥è³‡æ–™...\")\n",
        "df = pd.read_csv('AI_Human.csv', nrows=100000)\n",
        "print(f\"âœ… è¼‰å…¥ {len(df)} ç­†è³‡æ–™\")\n",
        "\n",
        "# æª¢æŸ¥æ¬„ä½\n",
        "if 'text' not in df.columns or 'generated' not in df.columns:\n",
        "    if len(df.columns) >= 2:\n",
        "        df.columns = ['text', 'generated'] + list(df.columns[2:])\n",
        "\n",
        "df = df.dropna(subset=['text', 'generated'])\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# è½‰æ›æ¨™ç±¤\n",
        "if df['generated'].dtype == bool:\n",
        "    df['label'] = df['generated'].astype(int)\n",
        "elif df['generated'].dtype == object:\n",
        "    df['label'] = df['generated'].apply(\n",
        "        lambda x: 1 if str(x).lower() in ['true', '1', 'ai', 'yes'] else 0\n",
        "    )\n",
        "else:\n",
        "    df['label'] = df['generated'].astype(int)\n",
        "\n",
        "print(f\"ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# é è™•ç†\n",
        "print(\"ğŸ”§ é è™•ç†è³‡æ–™...\")\n",
        "df = preprocess_dataframe(df, text_column='text', remove_stopwords=False)\n",
        "\n",
        "# åˆ†å‰²è³‡æ–™\n",
        "X = df['text'].values\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.67, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š è³‡æ–™åˆ†å‰²: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "# TF-IDF å‘é‡åŒ–\n",
        "print(\"ğŸ”¤ å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"âœ… TF-IDF ç‰¹å¾µç¶­åº¦: {X_train_tfidf.shape[1]}\")\n",
        "\n",
        "# è¨“ç·´æ¨¡å‹\n",
        "print(\"ğŸ‹ï¸  è¨“ç·´ SVM (C=1.0, kernel=rbf)...\")\n",
        "print(\"â³ é€™å¯èƒ½éœ€è¦ 5-30 åˆ†é˜...\")\n",
        "\n",
        "start_time = time.time()\n",
        "model = SVC(C=1.0, kernel='rbf', probability=True, random_state=42, verbose=True)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"âœ… è¨“ç·´å®Œæˆï¼è€—æ™‚: {elapsed_time/60:.2f} åˆ†é˜\")\n",
        "\n",
        "# è©•ä¼°\n",
        "print(\"ğŸ“Š è©•ä¼°æ¨¡å‹...\")\n",
        "y_val_pred = model.predict(X_val_tfidf)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"âœ… é©—è­‰é›†æº–ç¢ºç‡: {val_accuracy:.4f}\")\n",
        "\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"âœ… æ¸¬è©¦é›†æº–ç¢ºç‡: {test_accuracy:.4f}\")\n",
        "\n",
        "# å„²å­˜æ¨¡å‹\n",
        "print(\"ğŸ’¾ å„²å­˜æ¨¡å‹...\")\n",
        "!mkdir -p models/tfidf_svm\n",
        "\n",
        "with open('models/tfidf_svm/model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open('models/tfidf_svm/vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "# å„²å­˜ metrics\n",
        "metrics = {\n",
        "    \"model_name\": \"tfidf_svm\",\n",
        "    \"baseline_accuracy\": float(test_accuracy),\n",
        "    \"prompt_A_accuracy\": float(test_accuracy),\n",
        "    \"prompt_B_accuracy\": float(test_accuracy),\n",
        "    \"prompt_C_accuracy\": float(test_accuracy),\n",
        "    \"validation_accuracy\": float(val_accuracy),\n",
        "    \"test_accuracy\": float(test_accuracy)\n",
        "}\n",
        "\n",
        "with open('models/tfidf_svm/metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(\"âœ… æ¨¡å‹å·²å„²å­˜è‡³ models/tfidf_svm\")\n",
        "\n",
        "# ä¸‹è¼‰æ¨¡å‹\n",
        "from google.colab import files\n",
        "!zip -r models_tfidf_svm.zip models/tfidf_svm/\n",
        "files.download('models_tfidf_svm.zip')\n",
        "print(\"\\nâœ… æ¨¡å‹æª”æ¡ˆå·²æº–å‚™ä¸‹è¼‰ï¼\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
