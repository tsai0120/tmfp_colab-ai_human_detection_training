"""
TF-IDF + SVM æ¨¡å‹è¨“ç·´ç®¡ç·š
"""

import os
import sys
import pickle
import json
import time
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("ğŸ’¡ æç¤º: å®‰è£ tqdm å¯é¡¯ç¤ºé€²åº¦æ¢ (pip install tqdm)")

# åŠ å…¥ utils è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))
from utils.preprocessing import preprocess_dataframe


def load_data(data_path: str = "AI_Human.csv"):
    """è¼‰å…¥è³‡æ–™"""
    print(f"ğŸ“‚ è¼‰å…¥è³‡æ–™: {data_path}")
    
    # å˜—è©¦è®€å– CSV
    try:
        df = pd.read_csv(data_path, nrows=100000)  # é™åˆ¶è®€å–æ•¸é‡ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        print(f"âœ… è¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    except Exception as e:
        print(f"âŒ è¼‰å…¥å¤±æ•—: {e}")
        return None
    
    # æª¢æŸ¥æ¬„ä½
    if 'text' not in df.columns or 'generated' not in df.columns:
        print("âš ï¸  è³‡æ–™æ ¼å¼ä¸ç¬¦åˆé æœŸï¼Œå˜—è©¦è‡ªå‹•èª¿æ•´...")
        # å˜—è©¦æ‰¾åˆ°æ­£ç¢ºçš„æ¬„ä½
        if len(df.columns) >= 2:
            df.columns = ['text', 'generated'] + list(df.columns[2:])
    
    # æ¸…ç†è³‡æ–™
    df = df.dropna(subset=['text', 'generated'])
    df['text'] = df['text'].astype(str)
    
    # è½‰æ›æ¨™ç±¤ï¼šgenerated å¯èƒ½æ˜¯ 0/1, True/False, æˆ– 'AI'/'Human'
    if df['generated'].dtype == bool:
        df['label'] = df['generated'].astype(int)
    elif df['generated'].dtype == object:
        df['label'] = df['generated'].apply(
            lambda x: 1 if str(x).lower() in ['true', '1', 'ai', 'yes'] else 0
        )
    else:
        df['label'] = df['generated'].astype(int)
    
    print(f"ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {df['label'].value_counts().to_dict()}")
    
    return df[['text', 'label']]


def train_svm(
    data_path: str = "AI_Human.csv",
    model_dir: str = "models/tfidf_svm",
    test_size: float = 0.2,
    val_size: float = 0.1,
    C: float = 1.0,
    kernel: str = 'rbf',
    max_features: int = 5000,
    use_grid_search: bool = False
):
    """
    è¨“ç·´ TF-IDF + SVM æ¨¡å‹
    
    Args:
        data_path: è³‡æ–™è·¯å¾‘
        model_dir: æ¨¡å‹å„²å­˜ç›®éŒ„
        test_size: æ¸¬è©¦é›†æ¯”ä¾‹
        val_size: é©—è­‰é›†æ¯”ä¾‹
        C: SVM æ­£å‰‡åŒ–åƒæ•¸
        kernel: SVM kernel
        max_features: TF-IDF æœ€å¤§ç‰¹å¾µæ•¸
        use_grid_search: æ˜¯å¦ä½¿ç”¨ GridSearch
    """
    print("ğŸš€ é–‹å§‹è¨“ç·´ TF-IDF + SVM æ¨¡å‹...")
    
    # è¼‰å…¥è³‡æ–™
    df = load_data(data_path)
    if df is None:
        return
    
    # é è™•ç†
    print("ğŸ”§ é è™•ç†è³‡æ–™...")
    df = preprocess_dataframe(df, text_column='text', remove_stopwords=False)
    
    # åˆ†å‰²è³‡æ–™
    X = df['text'].values
    y = df['label'].values
    
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=test_size + val_size, random_state=42, stratify=y
    )
    
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=test_size / (test_size + val_size),
        random_state=42, stratify=y_temp
    )
    
    print(f"ğŸ“Š è³‡æ–™åˆ†å‰²: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
    
    # å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨
    print("ğŸ”¤ å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨...")
    vectorizer = TfidfVectorizer(
        max_features=max_features,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95
    )
    
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_val_tfidf = vectorizer.transform(X_val)
    X_test_tfidf = vectorizer.transform(X_test)
    
    print(f"âœ… TF-IDF ç‰¹å¾µç¶­åº¦: {X_train_tfidf.shape[1]}")
    
    # è¨“ç·´æ¨¡å‹
    if use_grid_search:
        print("ğŸ” ä½¿ç”¨ GridSearch å°‹æ‰¾æœ€ä½³åƒæ•¸...")
        print("â³ é€™å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
        start_time = time.time()
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf', 'poly']
        }
        svm = GridSearchCV(
            SVC(probability=True, random_state=42),
            param_grid,
            cv=3,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )
        svm.fit(X_train_tfidf, y_train)
        elapsed_time = time.time() - start_time
        print(f"âœ… æœ€ä½³åƒæ•¸: {svm.best_params_}")
        print(f"â±ï¸  è¨“ç·´æ™‚é–“: {elapsed_time/60:.2f} åˆ†é˜")
        model = svm.best_estimator_
    else:
        print(f"ğŸ‹ï¸  è¨“ç·´ SVM (C={C}, kernel={kernel})...")
        print(f"ğŸ“Š è¨“ç·´è³‡æ–™é‡: {len(X_train)} ç­†")
        print("â³ è¨“ç·´ä¸­ï¼Œé€™å¯èƒ½éœ€è¦ 5-30 åˆ†é˜ï¼ˆå–æ±ºæ–¼è³‡æ–™é‡å’Œ kernelï¼‰...")
        print("ğŸ’¡ æç¤º: RBF kernel è¼ƒæ…¢ä½†é€šå¸¸æ•ˆæœè¼ƒå¥½ï¼ŒLinear kernel è¼ƒå¿«")
        
        start_time = time.time()
        model = SVC(C=C, kernel=kernel, probability=True, random_state=42, verbose=True)
        
        # é¡¯ç¤ºé€²åº¦
        print("\n" + "="*50)
        print("é–‹å§‹è¨“ç·´...")
        print("="*50)
        
        model.fit(X_train_tfidf, y_train)
        
        elapsed_time = time.time() - start_time
        print("="*50)
        print(f"âœ… è¨“ç·´å®Œæˆï¼")
        print(f"â±ï¸  è¨“ç·´æ™‚é–“: {elapsed_time/60:.2f} åˆ†é˜ ({elapsed_time:.2f} ç§’)")
        print("="*50)
    
    # è©•ä¼°
    print("ğŸ“Š è©•ä¼°æ¨¡å‹...")
    
    # é©—è­‰é›†
    y_val_pred = model.predict(X_val_tfidf)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    print(f"âœ… é©—è­‰é›†æº–ç¢ºç‡: {val_accuracy:.4f}")
    
    # æ¸¬è©¦é›†
    y_test_pred = model.predict(X_test_tfidf)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    print(f"âœ… æ¸¬è©¦é›†æº–ç¢ºç‡: {test_accuracy:.4f}")
    
    # å¦‚æœæœ‰ prompt_type è³‡è¨Šï¼Œè¨ˆç®—å„ prompt çš„æº–ç¢ºç‡
    prompt_accuracies = {}
    baseline_accuracy = test_accuracy
    
    # å„²å­˜æ¨¡å‹
    print("ğŸ’¾ å„²å­˜æ¨¡å‹...")
    os.makedirs(model_dir, exist_ok=True)
    
    with open(f"{model_dir}/model.pkl", 'wb') as f:
        pickle.dump(model, f)
    
    with open(f"{model_dir}/vectorizer.pkl", 'wb') as f:
        pickle.dump(vectorizer, f)
    
    # å„²å­˜ metrics
    metrics = {
        "model_name": "tfidf_svm",
        "baseline_accuracy": float(baseline_accuracy),
        "prompt_A_accuracy": float(baseline_accuracy),  # é è¨­å€¼ï¼Œå¯å¾ŒçºŒæ›´æ–°
        "prompt_B_accuracy": float(baseline_accuracy),
        "prompt_C_accuracy": float(baseline_accuracy),
        "validation_accuracy": float(val_accuracy),
        "test_accuracy": float(test_accuracy),
        "parameters": {
            "C": float(C) if not use_grid_search else float(model.C),
            "kernel": kernel if not use_grid_search else model.kernel,
            "max_features": max_features
        }
    }
    
    with open(f"{model_dir}/metrics.json", 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³ {model_dir}")
    print(f"ğŸ“Š æœ€çµ‚æ¸¬è©¦æº–ç¢ºç‡: {test_accuracy:.4f}")
    
    return model, vectorizer, metrics


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¨“ç·´ TF-IDF + SVM æ¨¡å‹")
    parser.add_argument("--data", type=str, default="AI_Human.csv", help="è³‡æ–™è·¯å¾‘")
    parser.add_argument("--model_dir", type=str, default="models/tfidf_svm", help="æ¨¡å‹ç›®éŒ„")
    parser.add_argument("--C", type=float, default=1.0, help="SVM C åƒæ•¸")
    parser.add_argument("--kernel", type=str, default="rbf", choices=['linear', 'rbf', 'poly'], help="SVM kernel")
    parser.add_argument("--max_features", type=int, default=5000, help="TF-IDF æœ€å¤§ç‰¹å¾µæ•¸")
    parser.add_argument("--grid_search", action="store_true", help="ä½¿ç”¨ GridSearch")
    
    args = parser.parse_args()
    
    train_svm(
        data_path=args.data,
        model_dir=args.model_dir,
        C=args.C,
        kernel=args.kernel,
        max_features=args.max_features,
        use_grid_search=args.grid_search
    )

