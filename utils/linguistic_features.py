"""
èªè¨€ç‰¹å¾µæå–æ¨¡çµ„
è¨ˆç®—å„ç¨®èªè¨€å­¸ç‰¹å¾µç”¨æ–¼ AI vs Human æ–‡æœ¬åµæ¸¬
"""

import re
import math
import string
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

try:
    from transformers import GPT2LMHeadModel, GPT2Tokenizer, RobertaForMaskedLM, RobertaTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  transformers æœªå®‰è£ï¼Œå°‡è·³é perplexity è¨ˆç®—")


class LinguisticFeatureExtractor:
    """èªè¨€ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, enable_perplexity: bool = False):
        """
        Args:
            enable_perplexity: æ˜¯å¦å•Ÿç”¨ perplexity è¨ˆç®—ï¼ˆéœ€è¦ transformersï¼‰
        """
        self.enable_perplexity = enable_perplexity and TRANSFORMERS_AVAILABLE
        self.gpt2_model = None
        self.gpt2_tokenizer = None
        self.roberta_model = None
        self.roberta_tokenizer = None
        
        if self.enable_perplexity:
            self._load_perplexity_models()
    
    def _load_perplexity_models(self):
        """è¼‰å…¥ perplexity è¨ˆç®—æ¨¡å‹"""
        try:
            print("ğŸ“¥ è¼‰å…¥ GPT-2 æ¨¡å‹...")
            self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
            self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')
            self.gpt2_model.eval()
            
            print("ğŸ“¥ è¼‰å…¥ RoBERTa æ¨¡å‹...")
            self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
            self.roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')
            self.roberta_model.eval()
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è¼‰å…¥ perplexity æ¨¡å‹: {e}")
            self.enable_perplexity = False
    
    def type_token_ratio(self, text: str) -> float:
        """
        Type-Token Ratio (TTR)
        è©å½™å¤šæ¨£æ€§æŒ‡æ¨™
        """
        if not text or len(text.strip()) == 0:
            return 0.0
        
        try:
            tokens = word_tokenize(text.lower())
            if len(tokens) == 0:
                return 0.0
            unique_tokens = len(set(tokens))
            return unique_tokens / len(tokens)
        except:
            return 0.0
    
    def mean_sentence_length(self, text: str) -> float:
        """å¹³å‡å¥å­é•·åº¦ï¼ˆä»¥è©æ•¸è¨ˆç®—ï¼‰"""
        if not text or len(text.strip()) == 0:
            return 0.0
        
        try:
            sentences = sent_tokenize(text)
            if len(sentences) == 0:
                return 0.0
            
            lengths = []
            for sent in sentences:
                tokens = word_tokenize(sent)
                lengths.append(len(tokens))
            
            return np.mean(lengths) if lengths else 0.0
        except:
            return 0.0
    
    def burstiness(self, text: str) -> float:
        """
        Burstiness: è¡¡é‡æ–‡æœ¬ä¸­è©å½™å‡ºç¾çš„é›†ä¸­ç¨‹åº¦
        è¨ˆç®—æ–¹å¼ï¼šæ¨™æº–å·® / å¹³å‡æ•¸ï¼ˆå°å¥å­é•·åº¦ï¼‰
        """
        if not text or len(text.strip()) == 0:
            return 0.0
        
        try:
            sentences = sent_tokenize(text)
            if len(sentences) < 2:
                return 0.0
            
            lengths = []
            for sent in sentences:
                tokens = word_tokenize(sent)
                lengths.append(len(tokens))
            
            if np.mean(lengths) == 0:
                return 0.0
            
            return np.std(lengths) / np.mean(lengths) if lengths else 0.0
        except:
            return 0.0
    
    def punctuation_ratio(self, text: str) -> float:
        """æ¨™é»ç¬¦è™Ÿæ¯”ä¾‹"""
        if not text or len(text.strip()) == 0:
            return 0.0
        
        punctuation_chars = set(string.punctuation + 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        total_chars = len(text)
        punct_chars = sum(1 for char in text if char in punctuation_chars)
        
        return punct_chars / total_chars if total_chars > 0 else 0.0
    
    def character_entropy(self, text: str) -> float:
        """
        å­—å…ƒå±¤ç´šçš„ç†µï¼ˆEntropyï¼‰
        è¡¡é‡æ–‡æœ¬çš„éš¨æ©Ÿæ€§/è¤‡é›œåº¦
        """
        if not text or len(text.strip()) == 0:
            return 0.0
        
        char_counts = Counter(text.lower())
        total_chars = len(text)
        
        entropy = 0.0
        for count in char_counts.values():
            prob = count / total_chars
            if prob > 0:
                entropy -= prob * math.log2(prob)
        
        return entropy
    
    def gpt2_perplexity(self, text: str) -> Optional[float]:
        """ä½¿ç”¨ GPT-2 è¨ˆç®— perplexity"""
        if not self.enable_perplexity or not self.gpt2_model:
            return None
        
        try:
            inputs = self.gpt2_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.gpt2_model(**inputs, labels=inputs['input_ids'])
                loss = outputs.loss
                perplexity = math.exp(loss.item())
            return perplexity
        except Exception as e:
            print(f"âš ï¸  GPT-2 perplexity è¨ˆç®—å¤±æ•—: {e}")
            return None
    
    def roberta_perplexity(self, text: str) -> Optional[float]:
        """ä½¿ç”¨ RoBERTa è¨ˆç®— perplexity"""
        if not self.enable_perplexity or not self.roberta_model:
            return None
        
        try:
            inputs = self.roberta_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.roberta_model(**inputs, labels=inputs['input_ids'])
                loss = outputs.loss
                perplexity = math.exp(loss.item())
            return perplexity
        except Exception as e:
            print(f"âš ï¸  RoBERTa perplexity è¨ˆç®—å¤±æ•—: {e}")
            return None
    
    def extract_all_features(self, text: str) -> Dict[str, float]:
        """æå–æ‰€æœ‰ç‰¹å¾µ"""
        features = {
            "type_token_ratio": self.type_token_ratio(text),
            "mean_sentence_length": self.mean_sentence_length(text),
            "burstiness": self.burstiness(text),
            "punctuation_ratio": self.punctuation_ratio(text),
            "character_entropy": self.character_entropy(text)
        }
        
        if self.enable_perplexity:
            features["gpt2_perplexity"] = self.gpt2_perplexity(text)
            features["roberta_perplexity"] = self.roberta_perplexity(text)
        
        return features
    
    def extract_features_batch(self, texts: List[str], show_progress: bool = True) -> pd.DataFrame:
        """æ‰¹é‡æå–ç‰¹å¾µ"""
        results = []
        
        for i, text in enumerate(texts):
            if show_progress and (i + 1) % 100 == 0:
                print(f"ğŸ“Š è™•ç†é€²åº¦: {i + 1}/{len(texts)}")
            
            features = self.extract_all_features(str(text))
            results.append(features)
        
        return pd.DataFrame(results)


# æª¢æŸ¥ torch æ˜¯å¦å¯ç”¨ï¼ˆç”¨æ–¼ perplexityï¼‰
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    if TRANSFORMERS_AVAILABLE:
        print("âš ï¸  torch æœªå®‰è£ï¼Œperplexity åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")


if __name__ == "__main__":
    # æ¸¬è©¦
    extractor = LinguisticFeatureExtractor(enable_perplexity=False)
    
    test_text = """
    This is a sample text for testing linguistic features.
    It contains multiple sentences. Some are longer than others.
    We want to see how well our feature extraction works.
    """
    
    features = extractor.extract_all_features(test_text)
    print("æå–çš„ç‰¹å¾µ:")
    for key, value in features.items():
        print(f"  {key}: {value:.4f}")

